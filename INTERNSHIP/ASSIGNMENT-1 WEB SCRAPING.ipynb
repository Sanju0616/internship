{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e024e21d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 29) (1655440961.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 29\u001b[1;36m\u001b[0m\n\u001b[1;33m    This program sends an HTTP GET request to Wikipedia's main page, scrapes header tags (h1 to h6), extracts their text content, and then creates a DataFrame with the header texts. Finally, it prints the DataFrame.\u001b[0m\n\u001b[1;37m                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 29)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the Wikipedia page you want to scrape\n",
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all header tags (h1, h2, h3, h4, h5, h6)\n",
    "    header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    \n",
    "    # Extract the text content from the header tags\n",
    "    header_texts = [header.get_text() for header in header_tags]\n",
    "    \n",
    "    # Create a DataFrame from the header texts\n",
    "    df = pd.DataFrame({'Header Text': header_texts})\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n",
    "This program sends an HTTP GET request to Wikipedia's main page, scrapes header tags (h1 to h6), extracts their text content, and then creates a DataFrame with the header texts. Finally, it prints the DataFrame.\n",
    "\n",
    "Remember that web scraping may be subject to the terms of service of a website, so make sure to review and comply with Wikipedia's terms of use and robots.txt file before scraping their content.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "User\n",
    "how to make data frame\n",
    "ChatGPT\n",
    "To create a DataFrame in Python, you can use the Pandas library. DataFrames are a fundamental data structure for data manipulation and analysis. You can create a DataFrame from various data sources, including lists, dictionaries, CSV files, Excel files, SQL databases, and more. Here's a basic example of how to create a DataFrame from a list of dictionaries:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Create a list of dictionaries\n",
    "data = [\n",
    "    {'Name': 'Alice', 'Age': 25, 'City': 'New York'},\n",
    "    {'Name': 'Bob', 'Age': 30, 'City': 'Los Angeles'},\n",
    "    {'Name': 'Charlie', 'Age': 22, 'City': 'Chicago'},\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "In this example, we first import the Pandas library. Then, we create a list of dictionaries where each dictionary represents a row of data. The keys in the dictionaries become the column names of the DataFrame. Finally, we use pd.DataFrame(data) to create the DataFrame and print(df) to display it.\n",
    "\n",
    "You can also customize the column order and select specific columns when creating the DataFrame. Here's an example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "    {'Name': 'Alice', 'Age': 25, 'City': 'New York'},\n",
    "    {'Name': 'Bob', 'Age': 30, 'City': 'Los Angeles'},\n",
    "    {'Name': 'Charlie', 'Age': 22, 'City': 'Chicago'},\n",
    "]\n",
    "\n",
    "# Create a DataFrame with custom column order\n",
    "df = pd.DataFrame(data, columns=['Age', 'Name', 'City'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "This will create a DataFrame with columns in the order specified in the columns parameter.\n",
    "\n",
    "You can also create DataFrames from other data sources like CSV files, Excel files, SQL queries, or by reading data from various file formats. The Pandas library provides functions like pd.read_csv(), pd.read_excel(), and pd.read_sql() to read data from these sources and create DataFrames.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Is this conversation helpful so far?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "477580c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name  Age         City\n",
      "0    Alice   25     New York\n",
      "1      Bob   30  Los Angeles\n",
      "2  Charlie   22      Chicago\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a list of dictionaries\n",
    "data = [\n",
    "    {'Name': 'Alice', 'Age': 25, 'City': 'New York'},\n",
    "    {'Name': 'Bob', 'Age': 30, 'City': 'Los Angeles'},\n",
    "    {'Name': 'Charlie', 'Age': 22, 'City': 'Chicago'},\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c2ec11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n",
      "  Position            Team Matches Points Rating\n",
      "0        1        IndiaIND      49  5,839    119\n",
      "1        2    AustraliaAUS      36  4,015    112\n",
      "2        3     PakistanPAK      32  3,525    110\n",
      "3        4  South AfricaSA      29  3,166    109\n",
      "4        5   New ZealandNZ      38  4,007    105\n",
      "5        6      EnglandENG      34  3,377     99\n",
      "6        7     Sri LankaSL      43  3,943     92\n",
      "7        8   BangladeshBAN      40  3,574     89\n",
      "8        9  AfghanistanAFG      26  2,170     83\n",
      "9       10   West IndiesWI      38  2,582     68\n",
      "\n",
      "Top 10 ODI Batsmen:\n",
      "                                            Position                 Player  \\\n",
      "0                                               1(0)             Babar Azam   \n",
      "1                                               2(0)           Shubman Gill   \n",
      "2  3(3)This player has moved up in the rankings s...        Quinton de Kock   \n",
      "3  4(4)This player has moved up in the rankings s...       Heinrich Klaasen   \n",
      "4  5(2)This player has moved down in the rankings...           David Warner   \n",
      "5  =(2)This player has moved down in the rankings...            Virat Kohli   \n",
      "6                                               7(0)           Harry Tector   \n",
      "7  8(1)This player has moved up in the rankings s...           Rohit Sharma   \n",
      "8  9(4)This player has moved down in the rankings...  Rassie van der Dussen   \n",
      "9                                              10(0)            Imam-ul-Haq   \n",
      "\n",
      "  Team Rating  \n",
      "0  PAK    829  \n",
      "1  IND    823  \n",
      "2   SA    769  \n",
      "3   SA    756  \n",
      "4  AUS    747  \n",
      "5  IND    747  \n",
      "6  IRE    729  \n",
      "7  IND    725  \n",
      "8   SA    716  \n",
      "9  PAK    704  \n",
      "\n",
      "Top 10 ODI Bowlers:\n",
      "                                            Position          Player Team  \\\n",
      "0                                               1(0)  Josh Hazlewood  AUS   \n",
      "1                                               2(0)  Mohammed Siraj  IND   \n",
      "2  3(5)This player has moved up in the rankings s...  Keshav Maharaj   SA   \n",
      "3  4(1)This player has moved down in the rankings...     Rashid Khan  AFG   \n",
      "4  5(1)This player has moved down in the rankings...     Trent Boult   NZ   \n",
      "5  6(1)This player has moved down in the rankings...   Mohammad Nabi  AFG   \n",
      "6  7(1)This player has moved down in the rankings...      Adam Zampa  AUS   \n",
      "7  8(1)This player has moved down in the rankings...      Matt Henry   NZ   \n",
      "8                                               9(0)   Kuldeep Yadav  IND   \n",
      "9                                              10(0)  Shaheen Afridi  PAK   \n",
      "\n",
      "  Rating  \n",
      "0    670  \n",
      "1    668  \n",
      "2    656  \n",
      "3    654  \n",
      "4    653  \n",
      "5    641  \n",
      "6    635  \n",
      "7    634  \n",
      "8    632  \n",
      "9    625  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape and create a DataFrame\n",
    "def scrape_icc_rankings(url, columns):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the table containing the rankings\n",
    "        table = soup.find('table')\n",
    "\n",
    "        # Extract data from the table and create a list of dictionaries\n",
    "        data = []\n",
    "        for row in table.find_all('tr')[1:11]:\n",
    "            columns_data = row.find_all('td')\n",
    "            data_dict = {columns[i]: columns_data[i].get_text(strip=True) for i in range(len(columns))}\n",
    "            data.append(data_dict)\n",
    "\n",
    "        # Create a DataFrame from the list of dictionaries\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "# URL for the top 10 ODI teams\n",
    "odi_teams_url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "odi_teams_columns = ['Position', 'Team', 'Matches', 'Points', 'Rating']\n",
    "\n",
    "# URL for the top 10 ODI batsmen\n",
    "odi_batsmen_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "odi_batsmen_columns = ['Position', 'Player', 'Team', 'Rating']\n",
    "\n",
    "# URL for the top 10 ODI bowlers\n",
    "odi_bowlers_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "odi_bowlers_columns = ['Position', 'Player', 'Team', 'Rating']\n",
    "\n",
    "# Scrape and create DataFrames\n",
    "odi_teams_df = scrape_icc_rankings(odi_teams_url, odi_teams_columns)\n",
    "odi_batsmen_df = scrape_icc_rankings(odi_batsmen_url, odi_batsmen_columns)\n",
    "odi_bowlers_df = scrape_icc_rankings(odi_bowlers_url, odi_bowlers_columns)\n",
    "\n",
    "# Display DataFrames\n",
    "if odi_teams_df is not None:\n",
    "    print(\"Top 10 ODI Teams:\")\n",
    "    print(odi_teams_df)\n",
    "\n",
    "if odi_batsmen_df is not None:\n",
    "    print(\"\\nTop 10 ODI Batsmen:\")\n",
    "    print(odi_batsmen_df)\n",
    "\n",
    "if odi_bowlers_df is not None:\n",
    "    print(\"\\nTop 10 ODI Bowlers:\")\n",
    "    print(odi_bowlers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af929c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Women's ODI Teams:\n",
      "  Position            Team Matches Points Rating\n",
      "0        1    AustraliaAUS      19  3,084    162\n",
      "1        2      EnglandENG      23  2,991    130\n",
      "2        3  South AfricaSA      21  2,446    116\n",
      "3        4        IndiaIND      18  1,745     97\n",
      "4        5   New ZealandNZ      21  2,014     96\n",
      "5        6   West IndiesWI      18  1,610     89\n",
      "6        7     Sri LankaSL       9    714     79\n",
      "7        8   BangladeshBAN      11    816     74\n",
      "8        9     ThailandTHA      11    753     68\n",
      "9       10     PakistanPAK      21  1,435     68\n",
      "\n",
      "Top 10 Women's ODI Batsmen:\n",
      "  Position                Player Team Rating\n",
      "0     1(0)  Natalie Sciver-Brunt  ENG    807\n",
      "1     2(0)           Beth Mooney  AUS    750\n",
      "2     3(0)   Chamari Athapaththu   SL    736\n",
      "3     4(0)       Laura Wolvaardt   SA    727\n",
      "4     5(0)       Smriti Mandhana  IND    708\n",
      "5     6(0)          Alyssa Healy  AUS    698\n",
      "6     7(0)          Ellyse Perry  AUS    697\n",
      "7     8(0)      Harmanpreet Kaur  IND    694\n",
      "8     9(0)           Meg Lanning  AUS    662\n",
      "9    10(0)        Marizanne Kapp   SA    642\n",
      "\n",
      "Top 10 Women's ODI All-rounders:\n",
      "  Position                Player Team Rating\n",
      "0     1(0)        Marizanne Kapp   SA    385\n",
      "1     2(0)      Ashleigh Gardner  AUS    377\n",
      "2     3(0)  Natalie Sciver-Brunt  ENG    360\n",
      "3     4(0)       Hayley Matthews   WI    358\n",
      "4     5(0)           Amelia Kerr   NZ    346\n",
      "5     6(0)         Deepti Sharma  IND    312\n",
      "6     7(0)          Ellyse Perry  AUS    282\n",
      "7     8(0)         Jess Jonassen  AUS    227\n",
      "8     =(0)         Sophie Devine   NZ    227\n",
      "9    10(0)              Nida Dar  PAK    224\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape and create a DataFrame\n",
    "def scrape_icc_rankings(url, columns):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the table containing the rankings\n",
    "        table = soup.find('table')\n",
    "\n",
    "        # Extract data from the table and create a list of dictionaries\n",
    "        data = []\n",
    "        for row in table.find_all('tr')[1:11]:\n",
    "            columns_data = row.find_all('td')\n",
    "            data_dict = {columns[i]: columns_data[i].get_text(strip=True) for i in range(len(columns))}\n",
    "            data.append(data_dict)\n",
    "\n",
    "        # Create a DataFrame from the list of dictionaries\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "# URL for the top 10 Women's ODI teams\n",
    "women_odi_teams_url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "women_odi_teams_columns = ['Position', 'Team', 'Matches', 'Points', 'Rating']\n",
    "\n",
    "# URL for the top 10 Women's ODI batsmen\n",
    "women_odi_batsmen_url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "women_odi_batsmen_columns = ['Position', 'Player', 'Team', 'Rating']\n",
    "\n",
    "# URL for the top 10 Women's ODI all-rounders\n",
    "women_odi_allrounders_url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "women_odi_allrounders_columns = ['Position', 'Player', 'Team', 'Rating']\n",
    "\n",
    "# Scrape and create DataFrames\n",
    "women_odi_teams_df = scrape_icc_rankings(women_odi_teams_url, women_odi_teams_columns)\n",
    "women_odi_batsmen_df = scrape_icc_rankings(women_odi_batsmen_url, women_odi_batsmen_columns)\n",
    "women_odi_allrounders_df = scrape_icc_rankings(women_odi_allrounders_url, women_odi_allrounders_columns)\n",
    "\n",
    "# Display DataFrames\n",
    "if women_odi_teams_df is not None:\n",
    "    print(\"Top 10 Women's ODI Teams:\")\n",
    "    print(women_odi_teams_df)\n",
    "\n",
    "if women_odi_batsmen_df is not None:\n",
    "    print(\"\\nTop 10 Women's ODI Batsmen:\")\n",
    "    print(women_odi_batsmen_df)\n",
    "\n",
    "if women_odi_allrounders_df is not None:\n",
    "    print(\"\\nTop 10 Women's ODI All-rounders:\")\n",
    "    print(women_odi_allrounders_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c6586f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m times \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m news_links \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnews_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     25\u001b[0m     headline \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m     time \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the CNBC World news page\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the container that holds the news articles\n",
    "    news_container = soup.find('div', {'id': 'container'})\n",
    "\n",
    "    # Extract news details (headline, time, and news link)\n",
    "    headlines = []\n",
    "    times = []\n",
    "    news_links = []\n",
    "\n",
    "    for article in news_container.find_all('article'):\n",
    "        headline = article.find('h3').get_text(strip=True)\n",
    "        time = article.find('time').get('datetime')\n",
    "        news_link = 'https://www.cnbc.com' + article.find('a').get('href')\n",
    "\n",
    "        headlines.append(headline)\n",
    "        times.append(time)\n",
    "        news_links.append(news_link)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    data = {\n",
    "        'Headline': headlines,\n",
    "        'Time': times,\n",
    "        'News Link': news_links\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4996218",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m published_dates \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     23\u001b[0m paper_urls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m \u001b[43marticles_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjs-article-list-item\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     26\u001b[0m     paper_title \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m     authors \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjs-article-list-authors\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the most downloaded articles page\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the container that holds the articles\n",
    "    articles_container = soup.find('div', class_='detail')\n",
    "\n",
    "    # Extract details (paper title, authors, published date, and paper URL)\n",
    "    paper_titles = []\n",
    "    authors_list = []\n",
    "    published_dates = []\n",
    "    paper_urls = []\n",
    "\n",
    "    for article in articles_container.find_all('li', class_='js-article-list-item'):\n",
    "        paper_title = article.find('h2').get_text(strip=True)\n",
    "        authors = article.find('p', class_='js-article-list-authors').get_text(strip=True)\n",
    "        published_date = article.find('p', class_='js-article-list-pub-date').get_text(strip=True)\n",
    "        paper_url = article.find('a', class_='js-article-list-title').get('href')\n",
    "\n",
    "        paper_titles.append(paper_title)\n",
    "        authors_list.append(authors)\n",
    "        published_dates.append(published_date)\n",
    "        paper_urls.append(paper_url)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    data = {\n",
    "        'Paper Title': paper_titles,\n",
    "        'Authors': authors_list,\n",
    "        'Published Date': published_dates,\n",
    "        'Paper URL': paper_urls\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d1efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the Dineout page\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants'\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the container that holds the restaurant details\n",
    "    restaurant_container = soup.find('div', class_='restaurant-list-tiles')\n",
    "\n",
    "    # Extract details (restaurant name, cuisine, location, ratings, and image URL)\n",
    "    restaurant_names = []\n",
    "    cuisines = []\n",
    "    locations = []\n",
    "    ratings = []\n",
    "    image_urls = []\n",
    "\n",
    "    for restaurant in restaurant_container.find_all('div', class_='restnt-info-main'):\n",
    "        name = restaurant.find('div', class_='restnt-info-dwn').h4.a.get_text(strip=True)\n",
    "        cuisine = restaurant.find('div', class_='restnt-info-dwn').find('span', class_='double-line-ellipsis').get_text(strip=True)\n",
    "        location = restaurant.find('div', class_='restnt-info-dwn').find('div', class_='loc-dtls').span.get_text(strip=True)\n",
    "        rating = restaurant.find('div', class_='rating').text\n",
    "        image_url = restaurant.find('div', class_='img-wraper').img['data-src']\n",
    "\n",
    "        restaurant_names.append(name)\n",
    "        cuisines.append(cuisine)\n",
    "        locations.append(location)\n",
    "        ratings.append(rating)\n",
    "        image_urls.append(image_url)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    data = {\n",
    "        'Restaurant Name': restaurant_names,\n",
    "        'Cuisine': cuisines,\n",
    "        'Location': locations,\n",
    "        'Ratings': ratings,\n",
    "        'Image URL': image_urls\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de67b95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
