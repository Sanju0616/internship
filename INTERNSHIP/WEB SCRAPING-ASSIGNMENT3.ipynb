{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e62f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product_name):\n",
    "    base_url = \"https://www.amazon.in/s\"\n",
    "    \n",
    "    # Set up parameters for the search query\n",
    "    params = {\n",
    "        \"k\": product_name,\n",
    "    }\n",
    "\n",
    "    # Send a GET request to Amazon with the search parameters\n",
    "    response = requests.get(base_url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract product details (you may need to inspect the Amazon page to get the appropriate HTML tags)\n",
    "        products = soup.find_all('div', class_='s-include-content-margin')\n",
    "\n",
    "        if not products:\n",
    "            print(\"No products found.\")\n",
    "        else:\n",
    "            print(f\"Found {len(products)} products related to '{product_name}':\\n\")\n",
    "\n",
    "            for index, product in enumerate(products, start=1):\n",
    "                product_title = product.find('span', class_='a-text-normal').text\n",
    "                product_price = product.find('span', class_='a-offscreen').text\n",
    "                product_link = \"https://www.amazon.in\" + product.find('a', class_='a-link-normal')['href']\n",
    "\n",
    "                print(f\"{index}. {product_title}\\n   Price: {product_price}\\n   Link: {product_link}\\n\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Amazon search results. Status code: {response.status_code}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get user input for the product to be searched\n",
    "    user_input = input(\"guitar: \")\n",
    "    \n",
    "    # Call the search_amazon function with the user input\n",
    "    search_amazon(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeba968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_product_details(product):\n",
    "    brand_name = product.find('span', class_='a-size-base-plus').text.strip()\n",
    "    product_name = product.find('span', class_='a-text-normal').text.strip()\n",
    "    product_price = product.find('span', class_='a-offscreen').text.strip()\n",
    "\n",
    "    return {\n",
    "        \"Brand Name\": brand_name,\n",
    "        \"Name of the Product\": product_name,\n",
    "        \"Price\": product_price,\n",
    "        \"Return/Exchange\": product.find('div', class_='a-row a-size-base a-color-secondary').text.strip(),\n",
    "        \"Expected Delivery\": product.find('div', class_='a-row s-align-children-center').text.strip(),\n",
    "        \"Availability\": product.find('div', class_='a-row a-size-base a-color-secondary').text.strip(),\n",
    "        \"Product URL\": \"https://www.amazon.in\" + product.find('a', class_='a-link-normal')['href'],\n",
    "    }\n",
    "\n",
    "def search_amazon_and_scrape(product_name, num_pages=3):\n",
    "    base_url = \"https://www.amazon.in/s\"\n",
    "    all_products_data = []\n",
    "\n",
    "    for page in range(1, num_pages + 1):\n",
    "        params = {\"k\": product_name, \"page\": page}\n",
    "        response = requests.get(base_url, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            products = soup.find_all('div', class_='s-include-content-margin')\n",
    "\n",
    "            if not products:\n",
    "                print(f\"No products found on page {page}.\")\n",
    "                break\n",
    "\n",
    "            for product in products:\n",
    "                product_data = scrape_product_details(product)\n",
    "                all_products_data.append(product_data)\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to retrieve Amazon search results on page {page}. Status code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame(all_products_data)\n",
    "\n",
    "    # Replace missing values with \"-\"\n",
    "    df = df.fillna(\"-\")\n",
    "\n",
    "    # Save DataFrame to CSV file\n",
    "    df.to_csv(f\"{product_name}_amazon_products.csv\", index=False)\n",
    "\n",
    "    print(f\"Scraping and CSV file creation complete. Saved as {product_name}_amazon_products.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product to search on Amazon: \")\n",
    "    search_amazon_and_scrape(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_images(keyword, num_images=10):\n",
    "    # Create a directory for saving images\n",
    "    if not os.path.exists(keyword):\n",
    "        os.makedirs(keyword)\n",
    "\n",
    "    # Specify the path to your WebDriver (provide the path to your chromedriver executable)\n",
    "    driver_path = \"/path/to/chromedriver\"\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "    try:\n",
    "        # Open Google Images\n",
    "        driver.get(\"https://images.google.com/\")\n",
    "\n",
    "        # Find the search bar and search button\n",
    "        search_bar = driver.find_element(\"name\", \"q\")\n",
    "        search_button = driver.find_element(\"css selector\", \"input[value='Search']\")\n",
    "\n",
    "        # Keywords to search for\n",
    "        keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "        for keyword in keywords:\n",
    "            # Search for the keyword\n",
    "            search_bar.clear()\n",
    "            search_bar.send_keys(keyword)\n",
    "            search_button.click()\n",
    "\n",
    "            # Wait for results to load\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Scroll down to load more images\n",
    "            for _ in range(3):  # Scroll down three times\n",
    "                driver.find_element_by_tag_name(\"body\").send_keys(Keys.PAGE_DOWN)\n",
    "                time.sleep(1)\n",
    "\n",
    "            # Get image URLs from the search results\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            images = soup.find_all('img', class_='rg_i')\n",
    "\n",
    "            # Download the first num_images images\n",
    "            for i, image in enumerate(images[:num_images], start=1):\n",
    "                image_url = image.get('src')\n",
    "                image_path = os.path.join(keyword, f\"{keyword}_{i}.jpg\")\n",
    "\n",
    "                # Download the image\n",
    "                response = requests.get(image_url, stream=True)\n",
    "                with open(image_path, 'wb') as file:\n",
    "                    for chunk in response.iter_content(chunk_size=128):\n",
    "                        file.write(chunk)\n",
    "\n",
    "                print(f\"Downloaded {keyword} image {i}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72240608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ab0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_smartphone_details(product):\n",
    "    details = {}\n",
    "\n",
    "    details['Brand Name'] = product.find('div', {'class': '_4rR01T'}).text.strip()\n",
    "    details['Smartphone Name'] = product.find('a', {'class': '_1fQZEK'}).text.strip()\n",
    "\n",
    "    # Get details if available, otherwise set to \"-\"\n",
    "    details['Colour'] = product.find('a', {'class': '_1fQZEK'}).get('title', '-')\n",
    "    details['RAM'] = product.find('li', {'class': 'rgWa7D'}, text='RAM').find_next('li').text.strip() if product.find('li', {'class': 'rgWa7D'}, text='RAM') else '-'\n",
    "    details['Storage(ROM)'] = product.find('li', {'class': 'rgWa7D'}, text='Internal Storage').find_next('li').text.strip() if product.find('li', {'class': 'rgWa7D'}, text='Internal Storage') else '-'\n",
    "    details['Primary Camera'] = product.find('li', {'class': 'rgWa7D'}, text='Primary Camera').find_next('li').text.strip() if product.find('li', {'class': 'rgWa7D'}, text='Primary Camera') else '-'\n",
    "    details['Secondary Camera'] = product.find('li', {'class': 'rgWa7D'}, text='Secondary Camera').find_next('li').text.strip() if product.find('li', {'class': 'rgWa7D'}, text='Secondary Camera') else '-'\n",
    "    details['Display Size'] = product.find('li', {'class': 'rgWa7D'}, text='Display Size').find_next('li').text.strip() if product.find('li', {'class': 'rgWa7D'}, text='Display Size') else '-'\n",
    "    details['Battery Capacity'] = product.find('li', {'class': 'rgWa7D'}, text='Battery Capacity').find_next('li').text.strip() if product.find('li', {'class': 'rgWa7D'}, text='Battery Capacity') else '-'\n",
    "    details['Price'] = product.find('div', {'class': '_30jeq3 _1_WHN1'}).text.strip()\n",
    "    details['Product URL'] = \"https://www.flipkart.com\" + product.find('a', {'class': '_1fQZEK'})['href']\n",
    "\n",
    "    return details\n",
    "\n",
    "def search_flipkart_and_scrape(product_name):\n",
    "    base_url = \"https://www.flipkart.com/search\"\n",
    "    params = {\"q\": product_name}\n",
    "\n",
    "    # Send a GET request to Flipkart with the search parameters\n",
    "    response = requests.get(base_url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract smartphone details\n",
    "        products = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "        \n",
    "        if not products:\n",
    "            print(\"No products found.\")\n",
    "            return\n",
    "\n",
    "        # Scrape details for each smartphone\n",
    "        smartphone_details = [scrape_smartphone_details(product) for product in products]\n",
    "\n",
    "        # Create a DataFrame from the scraped data\n",
    "        df = pd.DataFrame(smartphone_details)\n",
    "\n",
    "        # Replace missing values with \"-\"\n",
    "        df = df.fillna(\"-\")\n",
    "\n",
    "        # Save DataFrame to CSV file\n",
    "        df.to_csv(f\"{product_name}_flipkart_smartphones.csv\", index=False)\n",
    "\n",
    "        print(f\"Scraping and CSV file creation complete. Saved as {product_name}_flipkart_smartphones.csv\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Flipkart search results. Status code: {response.status_code}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get user input for the smartphone to be searched\n",
    "    user_input = input(\"Enter the smartphone to search on Flipkart: \")\n",
    "\n",
    "    # Call the search_flipkart_and_scrape function with the user input\n",
    "    search_flipkart_and_scrape(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72310c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_coordinates(api_key, city_name):\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/place/findplacefromtext/json\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        'input': city_name,\n",
    "        'inputtype': 'textquery',\n",
    "        'fields': 'geometry/location',\n",
    "        'key': api_key,\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the Google Places API\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    if response.status_code == 200 and data.get('status') == 'OK':\n",
    "        # Extract latitude and longitude from the response\n",
    "        location = data['candidates'][0]['geometry']['location']\n",
    "        latitude = location['lat']\n",
    "        longitude = location['lng']\n",
    "\n",
    "        return latitude, longitude\n",
    "    else:\n",
    "        print(f\"Failed to retrieve coordinates. Status code: {response.status_code}, Error: {data.get('error_message', 'No error message')}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'YOUR_API_KEY' with your actual Google Cloud API key\n",
    "    api_key = 'YOUR_API_KEY'\n",
    "\n",
    "    # Get user input for the city to search\n",
    "    city_name = input(\"Enter the city name: \")\n",
    "\n",
    "    # Get coordinates using the Google Places API\n",
    "    coordinates = get_coordinates(api_key, city_name)\n",
    "\n",
    "    if coordinates:\n",
    "        print(f\"Coordinates for {city_name}: Latitude {coordinates[0]}, Longitude {coordinates[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f9dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    base_url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    \n",
    "    # Send a GET request to the Digit website\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract details of gaming laptops\n",
    "        laptops = soup.find_all('div', class_='TopNumbeHeading sticky-footer')\n",
    "\n",
    "        # Create a list to store laptop details\n",
    "        laptop_details = []\n",
    "\n",
    "        for laptop in laptops:\n",
    "            details = {}\n",
    "\n",
    "            details['Name'] = laptop.find('div', class_='TopNumbeHeading sticky-footer').text.strip()\n",
    "            details['Price'] = laptop.find('div', class_='rubric').text.strip()\n",
    "            details['Specifications'] = laptop.find('div', class_='Specs-Wrap').text.strip()\n",
    "\n",
    "            laptop_details.append(details)\n",
    "\n",
    "        # Create a DataFrame from the scraped data\n",
    "        df = pd.DataFrame(laptop_details)\n",
    "\n",
    "        # Save DataFrame to CSV file\n",
    "        df.to_csv(\"gaming_laptops_digit.csv\", index=False)\n",
    "\n",
    "        print(\"Scraping and CSV file creation complete. Saved as gaming_laptops_digit.csv\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Digit website. Status code: {response.status_code}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_gaming_laptops()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d807a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    base_url = \"https://www.forbes.com/billionaires/\"\n",
    "    \n",
    "    # Send a GET request to the Forbes website\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract details of billionaires\n",
    "        billionaires = soup.find_all('div', class_='person')\n",
    "\n",
    "        # Create a list to store billionaire details\n",
    "        billionaire_details = []\n",
    "\n",
    "        for billionaire in billionaires:\n",
    "            details = {}\n",
    "\n",
    "            details['Rank'] = billionaire.find('div', class_='rank').text.strip()\n",
    "            details['Name'] = billionaire.find('div', class_='name').text.strip()\n",
    "            details['Net Worth'] = billionaire.find('div', class_='netWorth').text.strip()\n",
    "            details['Age'] = billionaire.find('div', class_='age').text.strip()\n",
    "            details['Citizenship'] = billionaire.find('div', class_='countryOfCitizenship').text.strip()\n",
    "            details['Source'] = billionaire.find('div', class_='source').text.strip()\n",
    "            details['Industry'] = billionaire.find('div', class_='category').text.strip()\n",
    "\n",
    "            billionaire_details.append(details)\n",
    "\n",
    "        # Create a DataFrame from the scraped data\n",
    "        df = pd.DataFrame(billionaire_details)\n",
    "\n",
    "        # Save DataFrame to CSV file\n",
    "        df.to_csv(\"forbes_billionaires.csv\", index=False)\n",
    "\n",
    "        print(\"Scraping and CSV file creation complete. Saved as forbes_billionaires.csv\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Forbes website. Status code: {response.status_code}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_forbes_billionaires()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7358170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "def get_youtube_comments(api_key, video_id, max_comments=500):\n",
    "    youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "    try:\n",
    "        # Get video details\n",
    "        video_response = youtube.videos().list(\n",
    "            part=\"snippet\",\n",
    "            id=video_id\n",
    "        ).execute()\n",
    "\n",
    "        video_title = video_response['items'][0]['snippet']['title']\n",
    "\n",
    "        # Get video comments\n",
    "        comments = []\n",
    "        nextPageToken = None\n",
    "\n",
    "        while len(comments) < max_comments:\n",
    "            comment_response = youtube.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=nextPageToken\n",
    "            ).execute()\n",
    "\n",
    "            for item in comment_response['items']:\n",
    "                comment = item['snippet']['topLevelComment']['snippet']\n",
    "                comments.append({\n",
    "                    'author': comment['authorDisplayName'],\n",
    "                    'comment': comment['textDisplay'],\n",
    "                    'upvotes': comment['likeCount'],\n",
    "                    'timestamp': comment['publishedAt'],\n",
    "                })\n",
    "\n",
    "            if 'nextPageToken' in comment_response:\n",
    "                nextPageToken = comment_response['nextPageToken']\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return video_title, comments[:max_comments]\n",
    "\n",
    "    except HttpError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'YOUR_API_KEY' with your actual YouTube Data API key\n",
    "    api_key = 'YOUR_API_KEY'\n",
    "\n",
    "    # Replace 'VIDEO_ID' with the actual video ID you want to scrape comments from\n",
    "    video_id = 'VIDEO_ID'\n",
    "\n",
    "    result = get_youtube_comments(api_key, video_id)\n",
    "\n",
    "    if result:\n",
    "        video_title, comments = result\n",
    "        print(f\"Video Title: {video_title}\")\n",
    "        print(f\"Total Comments Extracted: {len(comments)}\\n\")\n",
    "\n",
    "        for i, comment in enumerate(comments, start=1):\n",
    "            print(f\"Comment {i}:\")\n",
    "            print(f\"Author: {comment['author']}\")\n",
    "            print(f\"Comment: {comment['comment']}\")\n",
    "            print(f\"Upvotes: {comment['upvotes']}\")\n",
    "            print(f\"Timestamp: {comment['timestamp']}\")\n",
    "            print(\"---------\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve comments.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_hostelworld_data():\n",
    "    base_url = \"https://www.hostelworld.com/s/7?city=London&dateFrom=2023-01-01&dateTo=2023-01-07&guests=1\"\n",
    "\n",
    "    # Send a GET request to Hostelworld\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract details of hostels\n",
    "        hostels = soup.find_all('div', class_='hostel-details')\n",
    "\n",
    "        for hostel in hostels:\n",
    "            # Extract data for each hostel\n",
    "            hostel_name = hostel.find('h2', class_='title').text.strip()\n",
    "            distance_from_city_center = hostel.find('span', class_='description').text.strip()\n",
    "            ratings = hostel.find('div', class_='score orange big').text.strip()\n",
    "            total_reviews = hostel.find('div', class_='reviews').text.strip().split()[0]\n",
    "            overall_reviews = hostel.find('div', class_='reviews').text.strip().split()[3]\n",
    "            privates_price = hostel.find('div', class_='price-col').find('span', class_='price').text.strip()\n",
    "            dorms_price = hostel.find('div', class_='price-col').find_all('span', class_='price')[1].text.strip()\n",
    "            facilities = ', '.join([fac.text.strip() for fac in hostel.find_all('li', class_='facility-badge')])\n",
    "            property_description = hostel.find('div', class_='rating-factors').find_next('p').text.strip()\n",
    "\n",
    "            # Print or store the extracted data\n",
    "            print(f\"Hostel Name: {hostel_name}\")\n",
    "            print(f\"Distance from City Centre: {distance_from_city_center}\")\n",
    "            print(f\"Ratings: {ratings}\")\n",
    "            print(f\"Total Reviews: {total_reviews}\")\n",
    "            print(f\"Overall Reviews: {overall_reviews}\")\n",
    "            print(f\"Privates From Price: {privates_price}\")\n",
    "            print(f\"Dorms From Price: {dorms_price}\")\n",
    "            print(f\"Facilities: {facilities}\")\n",
    "            print(f\"Property Description: {property_description}\")\n",
    "            print(\"=\" * 50)\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Hostelworld website. Status code: {response.status_code}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_hostelworld_data()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
